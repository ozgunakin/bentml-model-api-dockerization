{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451eced2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Save Model Using Pickle\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# Fit the model on training set\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56fef366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logistic_model_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logistic_model_service.py\n",
    "import pandas as pd\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.service.artifacts.common import PickleArtifact\n",
    "\n",
    "@env(pip_packages=[\"sklearn\"])\n",
    "@artifacts([PickleArtifact('model')])\n",
    "class LogisticModel(BentoService):\n",
    "    \"\"\"\n",
    "    A minimum prediction service exposing a Scikit-learn model\n",
    "    \"\"\"\n",
    "\n",
    "    @api(input=DataframeInput(), batch=True)\n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        An inference API named `predict` with Dataframe input adapter, which codifies\n",
    "        how HTTP requests or CSV files are converted to a pandas Dataframe object as the\n",
    "        inference API function input\n",
    "        \"\"\"\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c358931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-09 12:17:05,979] INFO - BentoService bundle 'LogisticModel:20210709121705_8D9D86' saved to: /home/ubuntu/bentoml/repository/LogisticModel/20210709121705_8D9D86\n"
     ]
    }
   ],
   "source": [
    "#import pickle to load dumped model\n",
    "import pickle\n",
    "\n",
    "# import the LogisticModel class defined above\n",
    "from logistic_model_service import LogisticModel\n",
    "\n",
    "\n",
    "# import dumped model\n",
    "filename = 'finalized_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "# Create a logistic model service instance\n",
    "logistic_model_service = LogisticModel()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "logistic_model_service .pack('model', loaded_model )\n",
    "\n",
    "# Save the prediction service to disk for model serving\n",
    "saved_path = logistic_model_service .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3e665a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-09 11:29:29,645] INFO - Getting latest version LogisticModel:20210709112532_463C04\n",
      "[2021-07-09 11:29:29,654] INFO - Starting BentoML API proxy in development mode..\n",
      "[2021-07-09 11:29:29,655] INFO - Starting BentoML API server in development mode..\n",
      "[2021-07-09 11:29:29,783] INFO - Micro batch enabled for API `predict` max-latency: 20000 max-batch-size 4000\n",
      "[2021-07-09 11:29:29,783] INFO - Your system nofile limit is 4096, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "======== Running on http://0.0.0.0:5000 ========\n",
      "(Press CTRL+C to quit)\n",
      " * Serving Flask app 'LogisticModel' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:42035/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [09/Jul/2021 11:34:21] \"\u001b[33mPOST /corporate HTTP/1.1\u001b[0m\" 404 -\n",
      "[2021-07-09 11:34:31,551] INFO - outbound function called: 1\n",
      "[2021-07-09 11:34:31,555] ERROR - Error caught in API function:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/bentoml/service/inference_api.py\", line 176, in wrapped_func\n",
      "    return self._user_func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/bentoml/repository/LogisticModel/20210709112532_463C04/LogisticModel/logistic_model_service.py\", line 20, in predict\n",
      "    return self.artifacts.model.predict(df)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/sklearn/linear_model/_base.py\", line 309, in predict\n",
      "    scores = self.decision_function(X)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/sklearn/linear_model/_base.py\", line 284, in decision_function\n",
      "    X = check_array(X, accept_sparse='csr')\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 687, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Expected 2D array, got scalar array instead:\n",
      "array=None.\n",
      "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
      "127.0.0.1 - - [09/Jul/2021 11:34:31] \"POST /predict HTTP/1.1\" 200 -\n",
      "[2021-07-09 11:35:14,623] INFO - outbound function called: 1\n",
      "[2021-07-09 11:35:14,630] INFO - {'service_name': 'LogisticModel', 'service_version': '20210709112532_463C04', 'api': 'predict', 'task': {'data': '[[3.0, 102.0, 44.0, 20.0, 94.0, 30.8, 0.4, 26]]', 'task_id': '1cebfdca-92bc-4c02-80cf-2226f195012c', 'batch': 1, 'http_headers': (('Host', 'localhost:5000'), ('User-Agent', 'python-requests/2.22.0'), ('Accept-Encoding', 'gzip, deflate'), ('Accept', '*/*'), ('Connection', 'keep-alive'), ('Content-Length', '47'), ('Content-Type', 'application/json'))}, 'result': {'data': '[0.0]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': '1cebfdca-92bc-4c02-80cf-2226f195012c'}\n",
      "127.0.0.1 - - [09/Jul/2021 11:35:14] \"POST /predict HTTP/1.1\" 200 -\n",
      "[2021-07-09 11:36:17,965] INFO - outbound function called: 1\n",
      "[2021-07-09 11:36:17,973] INFO - {'service_name': 'LogisticModel', 'service_version': '20210709112532_463C04', 'api': 'predict', 'task': {'data': '[[3, 102, 44, 20, 94, 30.8, 0.4, 26]]', 'task_id': '7ac5acd6-63b3-421b-8b16-48a892e1a4d5', 'batch': 1, 'http_headers': (('Host', 'localhost:5000'), ('User-Agent', 'python-requests/2.22.0'), ('Accept-Encoding', 'gzip, deflate'), ('Accept', '*/*'), ('Connection', 'keep-alive'), ('Content-Length', '37'), ('Content-Type', 'application/json'))}, 'result': {'data': '[0.0]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': '7ac5acd6-63b3-421b-8b16-48a892e1a4d5'}\n",
      "127.0.0.1 - - [09/Jul/2021 11:36:17] \"POST /predict HTTP/1.1\" 200 -\n",
      "[2021-07-09 11:39:04,846] INFO - outbound function called: 1\n",
      "[2021-07-09 11:39:04,852] INFO - {'service_name': 'LogisticModel', 'service_version': '20210709112532_463C04', 'api': 'predict', 'task': {'data': '[[3, 102, 44, 20, 94, 30.8, 0.4, 26]]', 'task_id': 'b3c606ed-8263-4bd7-bc95-83d216b21784', 'batch': 1, 'http_headers': (('Host', 'localhost:5000'), ('User-Agent', 'python-requests/2.22.0'), ('Accept-Encoding', 'gzip, deflate'), ('Accept', '*/*'), ('Connection', 'keep-alive'), ('Content-Length', '37'), ('Content-Type', 'application/json'))}, 'result': {'data': '[0.0]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': 'b3c606ed-8263-4bd7-bc95-83d216b21784'}\n",
      "127.0.0.1 - - [09/Jul/2021 11:39:04] \"POST /predict HTTP/1.1\" 200 -\n",
      "[2021-07-09 11:39:04,854] INFO - optimizer params updated: o_a: 0.003826, o_b: 0.003826, wait: 0.000094\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve LogisticModel:latest --port 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5b4bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-07-09 12:19:45,974] INFO - Getting latest version LogisticModel:20210709112532_463C04\n",
      "\u001b[39mFound Bento: /home/ubuntu/bentoml/repository/LogisticModel/20210709112532_463C04\u001b[0m\n",
      "Containerizing LogisticModel:20210709112532_463C04 with local YataiService and docker daemon from local environment/\u001b[32mBuild container image: logistic-model:latest\u001b[0m\n",
      "\b \r"
     ]
    }
   ],
   "source": [
    "!bentoml containerize LogisticModel:latest -t logistic-model:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6296f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 5000:5000 logistic-model:latest --workers=1 --enable-microbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1505a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
